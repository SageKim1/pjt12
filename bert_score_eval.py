import os
from openai import OpenAI
import anthropic
import pandas as pd
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer, util
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
import itertools

# ===========================
# 0ï¸âƒ£ .env ë¡œë“œ
# ===========================
load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")

client = OpenAI(api_key=openai_api_key)
anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)

# ===========================
# 1ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
# ===========================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
bert_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ===========================
# 2ï¸âƒ£ ê³¼ëª©ëª… & ì§ˆë¬¸ ì„¤ì •
# ===========================
subject = "PLC"
question = "íƒ€ì´ë¨¸ Tì™€ ì¹´ìš´í„° Cì˜ ë””í´íŠ¸ê°’ê³¼ ì‚¬ìš© ë²”ìœ„ëŠ”?"

models = {
    "gpt-3.5-turbo": "openai",
    "gpt-4o": "openai",
    "claude-3-haiku-20240307": "anthropic",  # Claude 3.0
    "claude-3-5-sonnet-20241022": "anthropic"  # Claude 3.5
}

# ===========================
# 3ï¸âƒ£ FAISS ë¬¸ì„œ ê²€ìƒ‰ (RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„±)
# ===========================
faiss_path = f"./faiss_subjects/{subject}"
vs = FAISS.load_local(faiss_path, embedding_model, allow_dangerous_deserialization=True)
docs = vs.similarity_search(question, k=3)
rag_context = "\n\n".join([doc.page_content for doc in docs])
print("\n[ğŸ” RAG ê²€ìƒ‰ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°]\n", rag_context[:500], "...\n")

# ===========================
# 4ï¸âƒ£ API í˜¸ì¶œ (RAG & Non-RAG)
# ===========================
rag_responses = {}
non_rag_responses = {}

for model_name, provider in models.items():
    # ---- Non-RAG ----
    print(f"[ìš”ì²­ ì¤‘] {model_name} ({provider}) - Non-RAG")
    if provider == "openai":
        completion = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": question}],
            timeout=60
        )
        non_rag_responses[model_name] = completion.choices[0].message.content
    else:
        completion = anthropic_client.messages.create(
            model=model_name,
            max_tokens=500,
            messages=[{"role": "user", "content": question}],
            timeout=60
        )
        non_rag_responses[model_name] = completion.content[0].text
    print(f"[ì™„ë£Œ] {model_name} - Non-RAG")

    # ---- RAG ----
    rag_prompt = f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n\në¬¸ì„œ:\n{rag_context}\n\nì§ˆë¬¸: {question}"
    print(f"[ìš”ì²­ ì¤‘] {model_name} ({provider}) - RAG")
    if provider == "openai":
        completion = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": rag_prompt}],
            timeout=60
        )
        rag_responses[model_name] = completion.choices[0].message.content
    else:
        completion = anthropic_client.messages.create(
            model=model_name,
            max_tokens=500,
            messages=[{"role": "user", "content": rag_prompt}],
            timeout=60
        )
        rag_responses[model_name] = completion.content[0].text
    print(f"[ì™„ë£Œ] {model_name} - RAG")

# ===========================
# 5ï¸âƒ£ BERTScore ê³„ì‚°
# ===========================
results = []

# ---- (A) RAG vs Non-RAG ----
for model_name in models.keys():
    emb_rag = bert_model.encode(rag_responses[model_name], convert_to_tensor=True)
    emb_non = bert_model.encode(non_rag_responses[model_name], convert_to_tensor=True)
    score = util.cos_sim(emb_rag, emb_non).item()
    results.append({
        "Comparison": "RAG vs Non-RAG (Same Model)",
        "Model": model_name,
        "Score": score,
        "RAG Response": rag_responses[model_name],
        "Non-RAG Response": non_rag_responses[model_name]
    })

# ---- (B) RAGë¼ë¦¬ ìœ ì‚¬ë„ ----
for m1, m2 in itertools.combinations(models.keys(), 2):
    emb1 = bert_model.encode(rag_responses[m1], convert_to_tensor=True)
    emb2 = bert_model.encode(rag_responses[m2], convert_to_tensor=True)
    score = util.cos_sim(emb1, emb2).item()
    results.append({
        "Comparison": "RAG vs RAG",
        "Model Pair": f"{m1} vs {m2}",
        "Score": score
    })

# ---- (C) Non-RAGë¼ë¦¬ ìœ ì‚¬ë„ ----
for m1, m2 in itertools.combinations(models.keys(), 2):
    emb1 = bert_model.encode(non_rag_responses[m1], convert_to_tensor=True)
    emb2 = bert_model.encode(non_rag_responses[m2], convert_to_tensor=True)
    score = util.cos_sim(emb1, emb2).item()
    results.append({
        "Comparison": "Non-RAG vs Non-RAG",
        "Model Pair": f"{m1} vs {m2}",
        "Score": score
    })

# ===========================
# 6ï¸âƒ£ CSV ì €ì¥
# ===========================
df_scores = pd.DataFrame(results)
df_scores.to_csv(f"bert_scores_comparison_{subject}.csv", index=False, encoding="utf-8-sig")
print(f"\nâœ… RAG/Non-RAG, RAGë¼ë¦¬, Non-RAGë¼ë¦¬ ìœ ì‚¬ë„ ê²°ê³¼ê°€ 'bert_scores_comparison_{subject}.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
