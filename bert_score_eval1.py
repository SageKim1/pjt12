import os
from openai import OpenAI
import anthropic
import pandas as pd
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer, util
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ===========================
# 0ï¸âƒ£ .env ë¡œë“œ
# ===========================
load_dotenv()

# ğŸ”‘ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
openai_api_key = os.getenv("OPENAI_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")

# OpenAI / Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=openai_api_key)
anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)

# ===========================
# 1ï¸âƒ£ ì„ë² ë”© ë° BERT ëª¨ë¸ ë¡œë“œ
# ===========================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
bert_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ===========================
# 2ï¸âƒ£ ê³¼ëª©ëª… & ì§ˆë¬¸ ì„¤ì •
# ===========================
subject = "PLC"
question = "melsec plc ë””ë°”ì´ìŠ¤ êµ¬ì„±ì— ëŒ€í•´ ì•Œë ¤ì¤˜"

# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
models = {
    "gpt-3.5-turbo": "openai",
    "gpt-4o": "openai",
    "claude-3-haiku-20240307": "anthropic",
    "claude-3-5-sonnet-20241022": "anthropic"
}

# ===========================
# 3ï¸âƒ£ FAISS ë¡œë“œ & ë¬¸ì„œ ê²€ìƒ‰
# ===========================
faiss_path = f"./faiss_subjects/{subject}"
vs = FAISS.load_local(faiss_path, embedding_model, allow_dangerous_deserialization=True)

docs = vs.similarity_search(question, k=3)
rag_context = "\n\n".join([doc.page_content for doc in docs])
print("\n[ğŸ” RAG ê²€ìƒ‰ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°]\n", rag_context[:500], "...\n")

# ===========================
# 4ï¸âƒ£ API í˜¸ì¶œ: Non-RAG & RAG
# ===========================
non_rag_responses = {}
rag_responses = {}

for model_name, provider in models.items():
    # ---- Non-RAG ì‘ë‹µ ----
    print(f"[ìš”ì²­ ì¤‘] {model_name} ({provider}) - Non-RAG")
    if provider == "openai":
        completion = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": question}],
            timeout=60  # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )
        non_rag_responses[model_name] = completion.choices[0].message.content
    else:
        completion = anthropic_client.messages.create(
            model=model_name,
            max_tokens=500,
            messages=[{"role": "user", "content": question}],
            timeout=60
        )
        non_rag_responses[model_name] = completion.content[0].text
    print(f"[ì™„ë£Œ] {model_name} - Non-RAG")

    # ---- RAG ì‘ë‹µ ----
    rag_prompt = f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n\në¬¸ì„œ:\n{rag_context}\n\nì§ˆë¬¸: {question}"
    print(f"[ìš”ì²­ ì¤‘] {model_name} ({provider}) - RAG")
    if provider == "openai":
        completion = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": rag_prompt}],
            timeout=60
        )
        rag_responses[model_name] = completion.choices[0].message.content
    else:
        completion = anthropic_client.messages.create(
            model=model_name,
            max_tokens=500,
            messages=[{"role": "user", "content": rag_prompt}],
            timeout=60
        )
        rag_responses[model_name] = completion.content[0].text
    print(f"[ì™„ë£Œ] {model_name} - RAG")

# ===========================
# 5ï¸âƒ£ BERTScore ê³„ì‚° ë° ì €ì¥
# ===========================
emb_q = bert_model.encode(question, convert_to_tensor=True)
results = []

for model_name in models.keys():
    score_non_rag = util.cos_sim(emb_q, bert_model.encode(non_rag_responses[model_name], convert_to_tensor=True)).item()
    score_rag = util.cos_sim(emb_q, bert_model.encode(rag_responses[model_name], convert_to_tensor=True)).item()

    results.append({
        "Model": model_name,
        "Type": "Non-RAG",
        "Score": score_non_rag,
        "Response": non_rag_responses[model_name]
    })
    results.append({
        "Model": model_name,
        "Type": "RAG",
        "Score": score_rag,
        "Response": rag_responses[model_name]
    })

    print(f"[{model_name}] Non-RAG: {score_non_rag:.4f} | RAG: {score_rag:.4f}")

# CSV ì €ì¥
df_scores = pd.DataFrame(results)
df_scores.to_csv(f"bert_scores_{subject}.csv", index=False, encoding="utf-8-sig")
print(f"\nâœ… BERTScore ë° ì‘ë‹µì´ 'bert_scores_{subject}.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
